<!-- markdown-toc start - Don't edit this section. Run M-x markdown-toc-generate-toc again -->
**Table of Contents**
* [简介](#简介)
* [基础](#基础)
	* [1 学习算法概念](#1-学习算法概念)
	* [2 容量与训练](#2-Softmax Regression)
	* [3 统计与估计](#3-Factoriztion Machine)
	* [4 监督与无监督](#4-SVM)
	* [5 随机梯度下降](#5-随机森林)
	* [6 构建与挑战](#6-构建与挑战)
* [实践](#模型)
	* [1 开始:深度前馈网络](#1-学习算法概念)
	* [2 正则化](#2-正则化)
	* [3 优化](#3-优化)
	* [4 调参](#4-调参)
* [应用](#应用)
	* [1 神经网络](#1-神经网络)
	* [2 CNN](#2-CNN)
	* [3 RNN](#3-RNN)
* [研究拓展](#研究拓展)

* [数学基础](#数学基础)


<!-- markdown-toc end -->

# 简介
基于GoodFellow的花书做笔记，把数学基础放到了最后面

# 基础
## 1 学习算法概念
学习算法: 任务T: 经验E->性能P
样本example:特征feature的集合
性能P:测试集、准确率/错误率
经验E:数据集/数据点，常用设计矩阵表示
## 2 容量与训练
泛化:在先前未观测的输入上表现良好的能力
训练误差/泛化误差
数据生成过程遵循"独立同分布假设"
容量的调整会导致:欠拟合/过拟合
表示容量:可选择的函数族
没有免费的午餐原理:没有一个算法总是比其他的要好
超参数:不是通过训练学习本身学习的值
验证集:训练算法观测不到的样本
## 3 统计与估计
贝叶斯误差:预先知道的真实预测出现的误差
点估计:良好的估计量的输出会接近生成训练数据的真实参数;函数估计是函数空间中的一个点估计
估计偏差:无偏/渐进无偏
<最大似然估计>机器学习首选估计方法;样本数目增到极大时，收敛率最好的渐进估计
<贝叶斯统计>使用真实参数的全分布;先验分布;训练数据有限时泛化很好
## 4 监督与无监督
无监督：很多特征的数据集找出有用的结构性质;主成分分析、k-means
监督：数据集中样本具有标签或目标，根据学习结果进行划分;概率监督学习、SVM(核技巧)、决策树
## 5 随机梯度下降
核心: 梯度是期望
流程: 在算法的每一步，均匀抽出训练集中的一小批量样本，使用梯度下降估计
代价: 样本量m的O(1)级别
## 6 构建与挑战
构建:特定的数据集(X,y)+代价函数+优化算法(求解代价函数梯度为零的正规方程)+模型
挑战:维数灾难/局部不变性和平滑正则化/流形
