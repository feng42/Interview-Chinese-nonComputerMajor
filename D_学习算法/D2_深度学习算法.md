<!-- markdown-toc start - Don't edit this section. Run M-x markdown-toc-generate-toc again -->
**Deep Learning**
* [简介](#简介)
* [基础](#基础)
	* [1 学习算法概念](#1-学习算法概念)
	* [2 容量与训练](#2-容量与训练)
	* [3 统计与估计](#3-统计与估计)
	* [4 监督与无监督](#4-监督与无监督)
	* [5 随机梯度下降](#5-随机梯度下降)
	* [6 构建与挑战](#6-构建与挑战)
* [实践](#模型)
	* [1 开始:深度前馈网络](#1-开始:深度前馈网络)
		* [模型基础](#模型基础)
		* [学习过程](#学习过程)
		* [架构设计](#架构设计)
		* [反向传播](#反向传播)
	* [2 正则化](#2-正则化)
		* [参数](#参数)
		* [数据](#数据)
		* [训练方案](#训练方案)
		* [模型构造](#模型构造)
		* [样本测试](#样本测试)
	* [3 优化](#3-优化)
	* [4 调参](#4-调参)
* [应用](#应用)
	* [1 神经网络](#1-神经网络)
	* [2 CNN](#2-CNN)
	* [3 RNN](#3-RNN)
* [研究拓展](#研究拓展)

* [数学基础](#数学基础)


<!-- markdown-toc end -->

# 简介
基于GoodFellow的花书做笔记，把数学基础放到了最后面


# 基础
## 1 学习算法概念
学习算法: 任务T: 经验E->性能P
样本example:特征feature的集合
性能P:测试集、准确率/错误率
经验E:数据集/数据点，常用设计矩阵表示
## 2 容量与训练
泛化:在先前未观测的输入上表现良好的能力
训练误差/泛化误差
数据生成过程遵循"独立同分布假设"
容量的调整会导致:欠拟合/过拟合
表示容量:可选择的函数族
没有免费的午餐原理:没有一个算法总是比其他的要好
超参数:不是通过训练学习本身学习的值
验证集:训练算法观测不到的样本
## 3 统计与估计
贝叶斯误差:预先知道的真实预测出现的误差
点估计:良好的估计量的输出会接近生成训练数据的真实参数;函数估计是函数空间中的一个点估计
估计偏差:无偏/渐进无偏
<最大似然估计>机器学习首选估计方法;样本数目增到极大时，收敛率最好的渐进估计
<贝叶斯统计>使用真实参数的全分布;先验分布;训练数据有限时泛化很好
## 4 监督与无监督
无监督：很多特征的数据集找出有用的结构性质;主成分分析、k-means
监督：数据集中样本具有标签或目标，根据学习结果进行划分;概率监督学习、SVM(核技巧)、决策树
## 5 随机梯度下降
核心: 梯度是期望
流程: 在算法的每一步，均匀抽出训练集中的一小批量样本，使用梯度下降估计
代价: 样本量m的O(1)级别
## 6 构建与挑战
构建:特定的数据集(X,y)+代价函数+优化算法(求解代价函数梯度为零的正规方程)+模型
挑战:维数灾难/局部不变性和平滑正则化/流形


# 实践
## 1 开始:深度前馈网络
### 1 模型基础
深度前馈网络，又称前馈神经网络或多层感知机（MLP）
前向:没有反馈
网络:链式结构  深度:链的全长  宽度:隐藏层的维数
激活函数:使前馈网络具有非线性
### 2 学习过程
神经网络非凸、初始值敏感->随机初始化、总是使用梯度下降
- 代价函数:训练数据和模型预测间的交叉熵->简化为统计量;梯度要有足够大，还要有好的预测性
- 输出单元:线性->高斯输出,sigmoid->Bernouli,softmax->Multinouli,对角精度矩阵,混合密度网络
- 隐藏单元
整流线性单元->绝对值整流、渗漏整流、参数化整流、maxout(抵抗灾难遗忘:忘记如何执行过去训练)
logistic sigmoid与tanh
[新的隐藏单元类型并不一定总是表现的显著更好]
径向基RBF、softplus、hard tanh
### 3 架构设计
万能近似定理:一个前馈神经网络如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数的隐藏层，只要给予网络足够数量的隐藏单元，可以任意精度来近似从一个有限维空间的Borel可测函数。需要的隐藏单元数量是O(2^n)的。
### 4 反向传播
反向传播:代价函数的信息通过网络向后流动，以便计算梯度;仅指用于计算梯度的方法
链式法则:<ASSETS>
递归实现:建立表记录中间值可以加快运算(类似于动态规划)

## 2 正则化
### 参数
1.  参数范数惩罚
参数范数惩罚:限制模型的学习能力,使模型偏好于[权值较小]的目标函数;通常只对权重做正则惩罚，因为权重比偏置的拟合权重多
本质:添加先验->限制模型参数分布->降低复杂度/抗干扰能力增强，防止过拟合
L2参数惩罚(岭回归):<ASSETS>
效果:在显著减小目标函数方向上的参数会保存比较好->感知到较高方差的输入
输入:如果输入参数的相关性较小，效果将更显著
L1参数惩罚(LASSO):<ASSETS>
效果:使一部分最优值的参数为0->稀疏
输入:被用于特征选择，更适合特征有关联的情况
2.  约束范数惩罚
通过增加或减小超参数alpha来大致扩大或收缩约束范围
显示约束和重投影:
- 先计算目标函数的下降步，再将参数重投影到最近点->不必多花时间寻找超参数
- 惩罚可能导致目标函数非凸，而重投影实现的显示约束不鼓励接近原点
- 防止权重无限制增加
实用:列范数的限制
### 数据
1.  数据集增强
解决数据量有限的问题;能对对象识别问题很有效;
方法:类别不改变，可以轻易模拟，如平移不变性
噪声:可以使模型对噪声更健壮;dropout
学习算法比对前提:相同的数据集增强方案
2.  噪声鲁棒性
用于输入:数据集增强
加入权重:表现权重的不确定性
输出目标:对噪声建模(滤波)，标签平滑
### 训练方案
1.  半监督学习
未标记样本和标记样本都被用于学习，可以通过共享参数实现
2.  多任务学习
共享通用参数，统计强度大大增强
3.  提前终止
定期评估，没有显著改善则终止
策略一:第二次训练再次初始化，使用第一轮提前终止确定的最佳步数在所有数据上进行训练
策略二:提前终止过拟合的目标值，保持参数，继续训练
### 模型表示
1.  参数绑定与共享
参数绑定：使两个模型接近
参数共享：强迫使用统一组参数，减少内存
2.  稀疏表示
不从参数的角度考虑，考虑惩罚激活单元
含有隐藏函数的模型本质上都能变得稀疏
3.  bagging
结合多个模型进行平均
4.  dropout
原理:丢弃非输出单元的子网络;大部分的模型没有被显式训练，训练子网络进行参数共享
提升:在有限内存下表示指数级模型;最泛用的隐式集成方法
### 样本测试
- 对抗训练
添加对抗样本进行训练;减少原有独立同分布测试集的错误率

## 3 优化
1.  机器学习的优化目标:
- 经验风险最小化:最小化平均训练误差;高容量的模型会记住训练集而导致过拟合，损失函数不一定有梯度可供优化
- 代理损失函数和提前终止:代理函数比原函数能学到的更多;提前终止用于真实损失函数获得收敛条件
- 批量和小批量:解决计算代价和训练集冗余问题;全训练集=批量,单个样本=随机/在线,介于之间=小批量/随机
2.  挑战:
病态:梯度很强但学习缓慢
局部极小值:可辨识性、可行解问题
高原、鞍点和其他平坦区域:代价函数平台但较高，需要梯度逃逸
悬崖和梯度爆炸:使用梯度截断
长期依赖:丧失学习到先前信息的能力
非精确梯度:使用代理损失函数来避免
结构间的弱对应:寻求良好初始点
理论限制
3.  基本算法
- 随机梯度下降<ASSETS>
- 动量<ASSETS>
- Nesterov动量<ASSETS>
4.  参数初始化
特性:破坏对称性;常用随机初始化权重
正则化观点:权重小一点防止过拟合
优化观点:权重大一点以成功传播信息
标准初始化/稀疏初始化
5.  自适应算法:
AdaGrad:参数偏导越大，学习率下降越快
RMSProp:使用移动平均引入超参数，控制平均的长度范围;有结合Nesterov动量的形式
Adam:动量并入梯度估计，偏置修正
6.  二阶近似方法:
牛顿法/共轭梯度/BFGS
7.  优化策略与元算法:
- 批标准化:自适应重参数化<ASSETS>
- 坐标下降:循环对单一子部分最小化，以达到局部最小值<ASSETS>
- Polyak平均:平均优化算法在参数空间访问轨迹中的几个点;用于大量数值应用
- 监督预训练:直接训练目标模型求解问题之前，训练简单模型求解简化问题;普遍使用贪心算法
- 延拓法:挑选初始点，构造一系列相同参数的目标函数，使优化更容易
- 课程学习:基于规划学习思想;首先学习简化概念，然后学习依赖于简单概念的复杂概念

